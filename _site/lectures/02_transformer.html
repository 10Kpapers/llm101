<ul>
  <li>NLP中Encoder-Decoder框架和注意力机制(Attention)</li>
  <li>阅读论文 Attention is All you Need 熟悉Transformer</li>
  <li>代码实践：
    <ul>
      <li>阅读The Annotated Transformer代码</li>
      <li>CMU机器翻译课程 <a href="https://www.phontron.com/class/mtandseq2seq2019/assignments.html">作业1</a></li>
      <li>CS224N作业3 <a href="https://web.stanford.edu/class/cs224n/assignments/a3_spr24_student_handout.pdf">Neural Machine Translation with RNN</a></li>
      <li>CS224N 作业4: <a href="https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf">Self-Attention, Transformers, and Pretraining</a></li>
      <li>CS336 作业1 <a href="https://github.com/stanford-cs336/spring2024-assignment1-basics">Building a Transformer LM</a></li>
    </ul>
  </li>
</ul>

<!-- **Suggested Readings:**
- [Readings 1](http://example.com)
- [Readings 2](http://example.com) -->
