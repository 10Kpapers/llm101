<ul>
  <li>回顾机器翻译任务的发展历史：统计机器翻译(SMT)、Encoder-Decoder结构、注意力(Attention)机制、BPE算法</li>
  <li>Transformer模型</li>
  <li>编程实践：
    <ul>
      <li>RNN Encoder-Decoder with Attention</li>
      <li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></li>
      <li>训练基于Transformer的中英翻译模型</li>
      <li>非代码：斯坦福CS224N 作业4 <a href="https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf">Attention和Position Embeddings分析</a></li>
    </ul>
  </li>
</ul>

