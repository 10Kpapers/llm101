<ul>
  <li>GPT-1 &amp;&amp; GPT-2
    <ul>
      <li>NLP中的预训练-微调范式: CoVe、ELMo、ULMFiT、GPT-1、BERT、GPT-2</li>
      <li>GPT-1 &amp;&amp; GPT-2: Pre-traring LM + Large scale  ==&gt; zero-shot</li>
      <li>编程实践：<a href="https://github.com/10Kpapers/llm101_codes/tree/main/%E7%AC%AC%E4%B8%89%E7%AB%A0LLMPre-trainingandBeyond/3.1GPT-1-and-GPT-2">阅读GPT-1/GPT-2代码；训练124M GPT-2</a></li>
    </ul>
  </li>
  <li>Train-time Compute Scaling Laws
    <ul>
      <li>解析(Train-time Compute) Scaling Laws for LM, Empirically</li>
      <li>计算 GPT模型参数和FLOPs</li>
      <li>Kaplan Scaling Laws &amp;&amp; Chinchilla Scaling Laws</li>
      <li>编程实践：<a href="https://colab.research.google.com/drive/1uOUO-zqEpTmPJkeVfzZZzi8zCo94nGAM#scrollTo=YKILkuscd3Zu">Scaling Laws for MNIST</a></li>
    </ul>
  </li>
  <li>GPT-3 and Beyond
    <ul>
      <li>涌现、幻觉、位置编码、合成数据、提示工程、SLMs …
<!-- - Prompt Engineering放在LLM调用API的实践课程里面，提供openai api和deepseek api两个版本的colab notebook --></li>
    </ul>
  </li>
</ul>

<!-- **Suggested Readings:** -->
<!-- - [Readings 1](http://example.com)
- [Readings 2](http://example.com) -->
