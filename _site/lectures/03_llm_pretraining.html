<ul>
  <li>GPT-1 &amp;&amp; GPT-2
    <ul>
      <li>NLP中的预训练-微调范式: CoVe、ELMo、ULMFiT、GPT-1、BERT、GPT-2</li>
      <li>GPT-1 &amp;&amp; GPT-2: Pre-traring LM + Large scale  ==&gt; zero-shot</li>
      <li>编程实践：<a href="https://github.com/10Kpapers/llm101_codes/tree/main/%E7%AC%AC%E4%B8%89%E7%AB%A0LLMPre-trainingandBeyond/3.1GPT-1-and-GPT-2">阅读GPT-1/GPT-2代码；训练124M GPT-2</a></li>
    </ul>
  </li>
  <li>Train-time Compute Scaling Laws
    <ul>
      <li>解析(Train-time Compute) Scaling Laws for LM, Empirically</li>
      <li>计算 GPT模型参数和FLOPs</li>
      <li>Kaplan Scaling Laws &amp;&amp; Chinchilla Scaling Laws</li>
      <li>编程实践：<a href="https://colab.research.google.com/drive/1uOUO-zqEpTmPJkeVfzZZzi8zCo94nGAM#scrollTo=YKILkuscd3Zu">Scaling Laws for MNIST</a></li>
    </ul>
  </li>
  <li>LLM预训练之分布式训练：数据并行、模型并行、混合精度训练、分布式通信
    <ul>
      <li>数据并行(Data Parallelism)
        <ul>
          <li>DDP(PyTorch), Sharded Data Parallelism: DeepSpeed-ZeRO</li>
          <li>分布式通信 (Distributed Communication)</li>
          <li>混合精度训练(Mixed Precision Training)和常见数据格式</li>
        </ul>
      </li>
      <li>模型并行(Model Parallelism)
        <ul>
          <li>张量并行(Tensor Parallelism, TP)</li>
          <li>流水线并行(Pipeline Parallelism, PP)</li>
          <li>序列并行(Sequence Parallelism, SP)</li>
          <li>上下文并行(Context Parallelism, CP)</li>
          <li>专家并行(Expert Parallelism, EP)</li>
          <li>分布式代码之<a href="https://github.com/huggingface/picotron">picotron</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>GPT-3 and Beyond
    <ul>
      <li>涌现、幻觉、位置编码、合成数据、提示工程、SLMs …
<!-- - Prompt Engineering放在LLM调用API的实践课程里面，提供openai api和deepseek api两个版本的colab notebook --></li>
    </ul>
  </li>
</ul>

<!-- **Suggested Readings:** -->
<!-- - [Readings 1](http://example.com)
- [Readings 2](http://example.com) -->
