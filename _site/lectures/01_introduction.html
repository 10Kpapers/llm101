<ul>
  <li>关于NLP的一些基础知识：NLP简介、了解常见的NLP任务、词向量(word2vec)、预训练模型(ELMo、BERT)发展历程</li>
  <li>回顾NLP中语言模型任务的发展历史：N-gram LM、FFN LM、RNN LM、GPT</li>
  <li>理解向量(Embedding)的重要性，<a href="https://github.com/MachineLovesLearning/llm101_codes">代码链接</a>：
    <ul>
      <li>词向量可视化</li>
      <li>调用SiliconFlow Embedding API 计算句子向量的余弦相似度</li>
      <li>基于transformers BERT fine-tuning的中文文本分类</li>
      <li>基于arXiv论文数据 + SiliconFlow API + faiss + streamlit 构建论文搜索引擎demo</li>
    </ul>
  </li>
  <li>一点数学计算：
    <ul>
      <li>斯坦福CS224N 作业2中<a href="https://web.stanford.edu/class/cs224n/assignments/a2.pdf">Understanding word2vec</a></li>
      <li>普林斯顿 COS 484 作业1中<a href="https://princeton-nlp.github.io/cos484/assignments/a1.pdf">LM和ppl理解</a></li>
    </ul>
  </li>
</ul>

<!-- **Suggested Readings:** -->
<!-- - [Readings 1](http://example.com) -->
<!-- - [Readings 2](http://example.com) -->
