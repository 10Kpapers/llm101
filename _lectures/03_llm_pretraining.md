---
type: lecture
date: 2024-11-18T8:00:00+4:30
title: LLM Pre-training and Beyong
tldr: "Short text to discribe what this lecture is about."
thumbnail: /static_files/presentations/lec.jpg

# 添加slides附件
links: 
     - url: /static_files/slides/第三章第一节GPT-1,GPT-2.2024.12.17.pdf
       name: "第三章第一节slides"

     - url: https://www.bilibili.com/video/BV1CPk5YRE35/?spm_id_from=333.1387.list.card_archive.click&vd_source=f390fbd44eabbd79d483210d5a4d770e
       name: "第三章第一节视频(bilibili)"
     
# links: 
#     - url: /static_files/presentations/lec.zip
#       name: notes
#     - url: /static_files/presentations/code.zip
#       name: codes
#     - url: https://google.com
#       name: slides
---
- GPT-1 && GPT-2
    - NLP中的预训练-微调范式: CoVe、ELMo、ULMFiT、GPT-1、BERT、GPT-2
    - GPT-1 && GPT-2: Pre-traring LM + Large scale  ==> zero-shot 
    - 编程实践：训练124M GPT-2
- GPT-3 and Beyond
    - Scaling Laws、涌现、幻觉、位置编码、合成数据、提示工程、SLMs ...
<!-- - Prompt Engineering放在LLM调用API的实践课程里面，提供openai api和deepseek api两个版本的colab notebook -->

<!-- **Suggested Readings:** -->
<!-- - [Readings 1](http://example.com)
- [Readings 2](http://example.com) -->
