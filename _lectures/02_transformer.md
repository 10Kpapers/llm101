---
type: lecture
date: 2024-12-02T8:00:00+4:30
title: Transformer模型
tldr: "Short text to discribe what this lecture is about."
thumbnail: /static_files/presentations/lec.jpg

# 添加slides附件
links: 
     - url: /static_files/slides/2-Transformer-2024.12.04.pdf
       name: "第二章slides"

     - url: https://www.bilibili.com/video/BV15S6NYjEXz/?spm_id_from=333.999.0.0
       name: "第一节和第二节视频(bilibili)"
     
     - url: https://www.bilibili.com/video/BV1TaiDYDEwo/?vd_source=f390fbd44eabbd79d483210d5a4d770e
       name: "第三节视频(bilibili)"
    #  https://www.bilibili.com/video/BV15S6NYjEXz/?spm_id_from=333.999.list.card_archive.click&vd_source=f390fbd44eabbd79d483210d5a4d770e
#     - url: /static_files/presentations/code.zip
#       name: codes
#     - url: https://google.com
#       name: slides
# links: 
#     - url: /static_files/presentations/lec.zip
#       name: notes
#     - url: /static_files/presentations/code.zip
#       name: codes
#     - url: https://google.com
#       name: slides
---
* 回顾机器翻译任务的发展历史：统计机器翻译(SMT)、Encoder-Decoder结构、注意力(Attention)机制、BPE算法
* Transformer模型
* 编程实践([代码链接](https://github.com/MachineLovesLearning/llm101_codes))：
  - RNN Encoder-Decoder with Attention
  - [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
  - 训练基于Transformer的中英翻译模型
  - 非代码：斯坦福CS224N 作业4 [Attention和Position Embeddings分析](https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf)
  
