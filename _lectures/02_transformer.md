---
type: lecture
date: 2024-11-17T8:00:00+4:30
title: Transformer模型
tldr: "Short text to discribe what this lecture is about."
thumbnail: /static_files/presentations/lec.jpg
# links: 
#     - url: /static_files/presentations/lec.zip
#       name: notes
#     - url: /static_files/presentations/code.zip
#       name: codes
#     - url: https://google.com
#       name: slides
---
* NLP中Encoder-Decoder框架和注意力机制(Attention)
* 阅读论文 Attention is All you Need 熟悉Transformer
* 代码实践：
  - 阅读The Annotated Transformer代码
  - CMU机器翻译课程 [作业1](https://www.phontron.com/class/mtandseq2seq2019/assignments.html)
  - CS224N作业3 [Neural Machine Translation with RNN](https://web.stanford.edu/class/cs224n/assignments/a3_spr24_student_handout.pdf)
  - CS224N 作业4: [Self-Attention, Transformers, and Pretraining](https://web.stanford.edu/class/cs224n/assignments/a4_spr24_student_handout.pdf)
  - CS336 作业1 [Building a Transformer LM](https://github.com/stanford-cs336/spring2024-assignment1-basics)


<!-- **Suggested Readings:**
- [Readings 1](http://example.com)
- [Readings 2](http://example.com) -->
